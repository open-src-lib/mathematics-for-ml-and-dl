# TODO

- Linear Algebra
  - Introduction to Linear Algebra
    - What is Linear Algebra?
    - Applications and Relevance of Linear Algebra in Machine Learning and Deep Learning
      - Linear Regression (Normal Equation & Least Squares Solution)
      - Principal Component Analysis (PCA) & Dimensionality Reduction
      - Neural Networks (Tensors, Weights as Matrices)
      - Computer Vision (Image as a Matrix, Convolutions)
      - Markov Chains & Stochastic Matrices
  - Vectors
    - Vector Operations
    - Addition & Subtraction
    - Scalar Multiplication
    - Dot Product (Scalar Product) (Mention how Dot Product is related to Inner Product to avoid future confusions)
    - Cross Product
    - Projection of Vectors
    - Angle Between Vectors & Cosine Similarity
    - Vector Spaces
      - Definition of Vector Space
      - Basis and Dimension
      - Span of Vectors
      - Linear Dependence and Linear Independence
      - Change of Basis
      - Vector Projections and Cosine Similarity
  - Matrices
    - Matrix Operations
      - Addition & Subtraction
      - Matrix Multiplication (Row by Column, Element-wise)
      - Inverse
      - Transpose
      - Matrix Factorization
        - Singular Value Decomposition (SVD)
    - Special Matrices
      - Identity Matrix
      - Diagonal Matrix
      - Symmetric Matrix
      - Orthogonal Matrix
      - Triangular Matrices (Upper & Lower)
      - Positive Definite & Positive Semi-Definite Matrices
      - Singular & Non-Singular Matrices
      - Sparse & Dense Matrices
  - Norms and Distance Metrics
    - Vector Norms
      - Norm (also known as Cardinality or L_0)
      - Euclidean Norm (L_2)
      - Manhattan Norm (L_1)
      - Max Norm (also known as Infinity Norm or L_∞)
      - p-Norm (L_p)
    - Matrix Norms
      - Frobenius Norm
      - Spectral Norm
    - Distance Metrics
      - Euclidean Distance
      - Manhattan Distance
      - Minkowski Distance
      - Cosine Similarity & Distance
  - Determinants
    - Properties of Determinants
    - Determinant of 2×2 and 3×3 Matrices
    - Cofactor Expansion (Laplace Expansion)
    - Determinants & Invertibility
  - Eigenvalues and Eigenvectors
    - Definition & Properties
    - Eigenvalue Decomposition (also known as Eigen Decomposition)
    - Diagonalization of a Matrix
    - Power Method for Finding Eigenvalues
    - Singular Value Decomposition (SVD)
  - Systems of Linear Equations
    - Solving Systems Using Matrices
    - Gaussian Elimination & Row Echelon Form (REF)
    - Gauss-Jordan Elimination & Reduced Row Echelon Form (RREF)
    - Cramer's Rule
    - Matrix Inversion Method for Solving Equations
  - Linear Transformations
    - Definition of a Linear Transformation
    - Matrix Representation of Linear Transformations
    - Rotation, Scaling, Reflection, Projection, and Shear Transformations
    - Kernel & Image of a Transformation
  - Tensors
    - Tensor Addition & Subtraction
    - Tensor Multiplication (Outer Product, Tensor Contraction)
    - Rank of a Tensor
    - Tensors in Machine Learning & Deep Learning

---

- Calculus
  - Limits
  - Derivatives
  - Chain Rule

- Statistics
  - Measures of Central Tendency

- Information Theory
  - KL Divergence

- Optimization Theory
  - Gradient Descent
